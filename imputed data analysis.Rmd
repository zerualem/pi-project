---
title: "Imputed data analysis"
author: "Zerihun Bekele"
date: "9/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Imputed data

| Data type | Imputed? | Method |
|----:|:----:|:----:|
| Static numeric | Yes | Mice |
| Braden activity | Yes | Mice |
| Time series binary | No | No missing values |
| time series numeric | Yes | Mice |
| Location | Yes | Mice |
| Periop numeric | Yes | Mice |
| Periop binary | NO | No missing values |
| Surgical data | No | Packages fail |


## Import imputed data

### Import and analyze static numeric data

```{r}
# Function to read and combine imputed data
combine_data <- function(fname){
  imp_dt <- readRDS(paste0("Mice-impute-output/", fname))
  imp1 <- complete(imp_dt, 1)
  imp2 <- complete(imp_dt, 2)
  imp3 <- complete(imp_dt, 3)

  cat_vars <- names(imp1)[unlist(lapply(imp1, is.factor))]
  num_vars <- setdiff(names(imp1), cat_vars)

  # Take the mean of the imputed tables for the numeric variables
  num_dt <- (imp1[,num_vars] + imp2[,num_vars] + imp3[, num_vars])/3

  # For the categorical data we can either pick at random or the most frequent one
  # Here I will just pick one at random
  # The the complete dataframe becomes the combination of the numerical and categorical
  return(cbind(num_dt, imp1[, cat_vars]))
}

num_vars <- function(df) names(df)[unlist(lapply(df, is.numeric))]

```

```{r message=FALSE}
library(mice)

static_combined <- combine_data("static_agg_datamice.rds")

```
## Check the plausibility of the data

```{r}
summary(static_combined)
```

The `Pi` column have to be replaced with the original data.

```{r}
static_agg <- read.csv("static_agg_data.csv", header = T, row.names = 1)
static_combined$pi <- static_agg$pi

# Replace NAs with 0
static_combined$pi[is.na(static_combined$pi)] <- 0
static_agg$pi <- static_combined$pi
table(static_combined$pi)
```

```{r fig.width=11, fig.height=11}
static_numeric <- static_combined[, num_vars(static_combined)]
static_agg_num <- static_agg[,num_vars(static_agg)]

distplot <- function(dt1, dt2){
  n.row <- ceiling(length(dt1)/5)
  par(mfrow=c(n.row,5), cex=0.7, cex.main=1.2,
      mai=c(5,2,2,2), 
      mar=c(2,2,2,.5))
  col1 <- yarrr::transparent("springgreen", trans.val = 0.2)
  col2 <- yarrr::transparent("red", trans.val = .6)
  for (i in seq(length(dt1)) ) {
    h1 <- hist(dt1[,i], breaks = 20, plot = F)
    h2 <- hist(dt2[,i], breaks = 20, plot = F)
    
    hist(dt1[,i], main = names(dt1)[i], freq=F, breaks = 20, ylim = c(0, 1.2*max(c(h1$density,h2$density))),
         col=col1, xlab = "", ylab = "")
    hist(dt2[,i], breaks = 20, freq=F, col=col2, xlab = "", ylab = "", add=T)
    box()
    
    #print(2*max(h1$counts,h2$counts, na.rm = T))
    
  }
}

#distplot(static_numeric)
distplot(static_agg_num, static_numeric)

```

## Kmeans clustering

Let us apply kmeans for the two groups
```{r}
library(caret)

# Convert to dummy variables (one-hot-encoding)
dmy <- dummyVars(" ~ .", data = static_combined, fullRank=T)
static_dmy <- data.frame(predict(dmy, newdata = static_combined))
names(static_dmy)

# Scale the data
static_scaled <- as.data.frame(scale(static_dmy), col.names = colnames(static_dmy))
```

```{r fig.width=10, fig.height=8}
library(corrplot)

stat_corr <- cor(static_scaled)

for (i in 1:ncol(stat_corr)) {
  stat_corr[i,i] <- 0
}
high_corr <- apply(stat_corr, 1, function (x) any(x>0.9))

par(oma=c(0.2,0.1,1,0.2))
corrplot(stat_corr[high_corr, high_corr], method="color", type = 'lower', addCoef.col = "black")
mtext("Variables with correlation > 0.9 with at least one variable", outer = TRUE, cex = 1.5)
```

## Kmeans clustering 

```{r message=FALSE, fig.width=10, fig.height=8}
library(dplyr)

k2 <- kmeans(select(static_scaled, -c(pi, pi_dayfromadmit) ), centers = 2, nstart = 10)


# plot the 2 clusters identified by kmeans
library(factoextra)
p1 <- fviz_cluster(k2, geom = "point", data = static_scaled) + ggtitle("k = 2")
p1
```

### Color based on actual pi values

```{r fig.width=10, fig.height=8}
proj <- data.frame(p1$data)
proj['pi'] <- static_combined$pi

p1 + geom_jitter(aes(x=x, y = y, col=factor(pi)), data= proj)
```

### Import and analyze other data

```{r}
ts_numeric <- combine_data("ts_numeric_aggmice.rds")
periop_numeric <- combine_data("periop_numeric_aggmice.rds")
braden_perc <- combine_data("braden_agg_percmice.rds")
periop_binary <- read.csv("periop_yn_agg.csv")
ts_binary <- read.csv("ts_binary_agg.csv")

all_piData <- cbind(static_combined, ts_numeric, ts_binary, periop_numeric, periop_binary, braden_perc)

dim(all_piData)

```

## Logistic regression

### Split the data to training and testing set

```{r}

which(apply(all_piData, 2, function(x) any(is.na(x))))
```

```{r fig.width=10, fig.height=8}
num_features <- lapply(all_piData, is.numeric)

pi_corr <- cor(all_piData[, names(which(unlist(num_features )))])

for (i in 1:ncol(pi_corr)) {
  pi_corr[i,i] <- 0
}
high_corr <- apply(pi_corr, 1, function (x) any(abs(x)>0.95))
sum(high_corr)
pi_hi_corr <- pi_corr[high_corr, high_corr]

corrplot(pi_hi_corr, method="color", tl.pos = "n")
```



```{r}
library(dplyr)
all_piData <- subset(all_piData, select = -periop_tempmax_avg)

# Create Training Data
pi_ones <- all_piData[which(all_piData$pi == 1), ]  # all 1's
pi_zeros <- all_piData[which(all_piData$pi == 0), ]   # all 0's
set.seed(100)  # for repeatability of samples
pi_ones_training_rows <- sample(1:nrow(pi_ones), 0.7*nrow(pi_ones))  # 1's for training
pi_zeros_training_rows <- sample(1:nrow(pi_zeros), 0.7*nrow(pi_zeros))  # 0's for training. Pick as many 0's as 1's
training_ones <- pi_ones[pi_ones_training_rows, ]  
training_zeros <- pi_zeros[pi_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's 

# Create Test Data
test_ones <- pi_ones[-pi_ones_training_rows, ]
test_zeros <- pi_zeros[-pi_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's 
```


```{r fig.width=12, fig.height=8}
library(glmnet)

rm_vars <- c("pi", "pi_dayfromadmit", "pressure_injury_poa")
x_training = trainingData[, - which(names(trainingData) %in% rm_vars)]
x_test = testData[, - which(names(testData) %in% rm_vars)]
```



```{r fig.width=12, fig.height=8}
cvlog = cv.glmnet(data.matrix(x_training), trainingData$pi, family = "binomial", type.measure = "class")

plot(cvlog)
```

```{r}
library(caret)
y_test_pred <- predict(cvlog, newx = data.matrix(x_test), s = "lambda.min", type = "class")
xtab <- table(as.factor(testData$pi), as.factor(y_test_pred))
confusionMatrix(xtab, positive ="1")
```
### Compare the above result with a dummy prediction

```{r}
y_dummy <- factor(rep("0",nrow(testData)), levels = c("0","1"))
table(y_dummy)
dummytab <- table(as.factor(testData$pi), y_dummy)
dummytab
confusionMatrix(dummytab, positive ="1")
```

```{r}
age <- (static_combined$ageinyears-mean(static_combined$ageinyears))/(max(static_combined$ageinyears) -min(static_combined$ageinyears))
par(mfrow=c(1,3))
plot(density(static_scaled$ageinyears))
plot(density(static_combined$ageinyears))
plot(density(age))
```




