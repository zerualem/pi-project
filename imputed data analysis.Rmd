---
title: "Imputed data analysis"
author: "Zerihun Bekele"
date: "9/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Imputed data

| Data type | Imputed? | Method |
|----:|:----:|:----:|
| Static numeric | Yes | Mice |
| Braden activity | Yes | Mice |
| Time series binary | No | No missing values |
| time series numeric | Yes | Mice |
| Location | Yes | Mice |
| Periop numeric | Yes | Mice |
| Periop binary | NO | No missing values |
| Surgical data | No | Packages fail |


## Import imputed data

### Import and analyze static numeric data

```{r}
# Function to read and combine imputed data
combine_data <- function(fname){
  imp_dt <- readRDS(paste0("Mice-impute-output/", fname))
  imp1 <- complete(imp_dt, 1)
  imp2 <- complete(imp_dt, 2)
  imp3 <- complete(imp_dt, 3)

  cat_vars <- names(imp1)[unlist(lapply(imp1, is.factor))]
  num_vars <- setdiff(names(imp1), cat_vars)

  # Take the mean of the imputed tables for the numeric variables
  num_dt <- (imp1[,num_vars] + imp2[,num_vars] + imp3[, num_vars])/3

  # For the categorical data we can either pick at random or the most frequent one
  # Here I will just pick one at random
  # The the complete dataframe becomes the combination of the numerical and categorical
  return(cbind(num_dt, imp1[, cat_vars]))
}

num_vars <- function(df) names(df)[unlist(lapply(df, is.numeric))]

```

```{r message=FALSE}
library(mice)

static_combined <- combine_data("static_agg_datamice.rds")

```
## Check the plausibility of the data

```{r}
summary(static_combined)
```

The `Pi` column have to be replaced with the original data.

```{r}
static_agg <- read.csv("static_agg_data.csv", header = T, row.names = 1)
static_combined$pi <- static_agg$pi

# Replace NAs with 0
static_combined$pi[is.na(static_combined$pi)] <- 0
static_agg$pi <- static_combined$pi
table(static_combined$pi)
```

```{r fig.width=11, fig.height=11}
static_numeric <- static_combined[, num_vars(static_combined)]
static_agg_num <- static_agg[,num_vars(static_agg)]

col1 <- yarrr::transparent("springgreen", trans.val = 0.2)
col2 <- yarrr::transparent("red", trans.val = .6)
  
distplot <- function(dt1, dt2){
  n.row <- ceiling(length(dt1)/5)
  par(mfrow=c(n.row,5), cex=0.7, cex.main=1.2,
      mai=c(5,2,2,2), 
      mar=c(2,2,2,.5))
  
  for (i in seq(length(dt1)) ) {
    h1 <- hist(dt1[,i], breaks = 20, plot = F)
    h2 <- hist(dt2[,i], breaks = 20, plot = F)
    
    hist(dt1[,i], main = names(dt1)[i], freq=F, breaks = 20, ylim = c(0, 1.2*max(c(h1$density,h2$density))),
         col=col1, xlab = "", ylab = "")
    hist(dt2[,i], breaks = 20, freq=F, col=col2, xlab = "", ylab = "", add=T)
    box()
    
    #print(2*max(h1$counts,h2$counts, na.rm = T))
    
  }
}

#distplot(static_numeric)
distplot(static_agg_num, static_numeric)
```

```{r fig.width=11, fig.height=11}
static_cat <- static_combined[, !(names(static_combined) %in% num_vars(static_combined))]
static_agg_cat <- static_agg[,!(names(static_agg) %in% num_vars(static_agg))]

bar_plot <- function(dt1, dt2){
  
  n.row <- ceiling(length(dt1)/8)
  par(mfrow=c(n.row,6), cex=0.7, cex.main=1.2,
      mai=c(5,2,2,2), 
      mar=c(2,2,2,.5))
  for (i in seq(ncol(dt1))) {
    x.prop1 <- prop.table(table(dt1[,i]))
    x.prop2 <- prop.table(table(dt2[,i]))
    
    barplot(x.prop1, xpd=FALSE, main = names(dt1)[i], 
              col = col1, 
              ylim = c(0,max(x.prop1,x.prop2)*1.1))
    barplot(x.prop2, xpd=F, col=col2, add = T)
    box()
  }
  
}

bar_plot(static_agg_cat, static_cat)
```


## Kmeans clustering

Let us apply kmeans for the two groups
```{r}
library(caret)

# Convert to dummy variables (one-hot-encoding)
dmy <- dummyVars(" ~ .", data = static_combined, fullRank=T)
static_dmy <- data.frame(predict(dmy, newdata = static_combined))
names(static_dmy)

# Scale the data
static_scaled <- as.data.frame(scale(static_dmy), col.names = colnames(static_dmy))

# min max scaling

min_max <- function(dt) {
  min_ <- apply(dt, 2, min, na.rm=T)
  max_ <- apply(dt, 2, max, na.rm=T)
  dt_min_max <- t(apply(apply(dt, 1, function(x) x-min_), 2, function(x) x/(max_ - min_)))
  return(dt_min_max)
}

static_min_max <- min_max(static_dmy)
summary(static_min_max)
```

```{r fig.width=10, fig.height=8}
library(corrplot)

stat_corr <- cor(static_min_max)

for (i in 1:ncol(stat_corr)) {
  stat_corr[i,i] <- 0
}
high_corr <- apply(stat_corr, 1, function (x) any(x>0.9))

par(oma=c(0.2,0.1,1,0.2))
corrplot(stat_corr[high_corr, high_corr], method="color", 
         type = 'lower', addCoef.col = "black")
mtext("Variables with correlation > 0.9 with at least one variable", 
      outer = TRUE, cex = 1.5)
```

## Kmeans clustering 

```{r message=FALSE, fig.width=10, fig.height=8}
library(dplyr)

km_dt <- min_max(select(static_dmy, -c(pi, pi_dayfromadmit) ))
k2 <- kmeans(km_dt, centers = 2, nstart = 10)

# plot the 2 clusters identified by kmeans
library(factoextra)
p1 <- fviz_cluster(k2, geom = "point", data = static_min_max) + ggtitle("k = 2")
p1
```

### Color based on actual pi values

```{r fig.width=10, fig.height=8}
proj <- data.frame(p1$data)
proj['pi'] <- static_combined$pi

ggplot() + geom_jitter(aes(x=x, y = y, col=factor(pi)), data= proj) +
  xlim(-15,0) + ylim(-10,10)
```

### PCA

```{r}
res.pca <- prcomp(km_dt)
fviz_eig(res.pca)

res.ind <- get_pca_ind(res.pca)
pca.cord <- data.frame(res.ind$coord)

```


```{r message=FALSE}
library(plotly)

k3 <- kmeans(km_dt, centers = 3, nstart = 10)

p <- plot_ly(pca.cord, x = ~Dim.1, y = ~Dim.2, z = ~Dim.3, color = as.factor(k3$cluster), colors = c('#8e24aa','#BF382A', '#0C4B8E')) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Dim1'),
                     yaxis = list(title = 'Dim 2'),
                     zaxis = list(title = 'Dim 3')))
p

```


### Best K value

```{r}
set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(km_dt, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- seq(2, 20,2)

# extract wss for 2-15 clusters
wss_values <- purrr::map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```
### Silhoutte method to determine k

```{r}
fviz_nbclust(km_dt, kmeans, method = "silhouette")
```

### t-SNE

```{r}
library(Rtsne)
set.seed(3434)
tsne_pi <- Rtsne(km_dt, dims = 2, perplexity=80, verbose=TRUE, max_iter = 500)
```
```{r}
pi.clr <- ifelse(static_combined$pi == 0, "#4caf50","#e64a19") 

plot(tsne_pi$Y,  col = pi.clr, )
```
## Logistic regression

### Split the data to training and testing set

```{r}

which(apply(all_piData, 2, function(x) any(is.na(x))))
```

```{r fig.width=10, fig.height=8}
num_features <- lapply(all_piData, is.numeric)

pi_corr <- cor(all_piData[, names(which(unlist(num_features )))])

for (i in 1:ncol(pi_corr)) {
  pi_corr[i,i] <- 0
}
high_corr <- apply(pi_corr, 1, function (x) any(abs(x)>0.95))
sum(high_corr)
pi_hi_corr <- pi_corr[high_corr, high_corr]

corrplot(pi_hi_corr, method="color", tl.pos = "n")
```



```{r}
library(dplyr)
all_piData <- subset(all_piData, select = -periop_tempmax_avg)

# Create Training Data
pi_ones <- all_piData[which(all_piData$pi == 1), ]  # all 1's
pi_zeros <- all_piData[which(all_piData$pi == 0), ]   # all 0's
set.seed(100)  # for repeatability of samples
pi_ones_training_rows <- sample(1:nrow(pi_ones), 0.7*nrow(pi_ones))  # 1's for training
pi_zeros_training_rows <- sample(1:nrow(pi_zeros), 0.7*nrow(pi_zeros))  # 0's for training. Pick as many 0's as 1's
training_ones <- pi_ones[pi_ones_training_rows, ]  
training_zeros <- pi_zeros[pi_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's 

# Create Test Data
test_ones <- pi_ones[-pi_ones_training_rows, ]
test_zeros <- pi_zeros[-pi_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's 
```


```{r fig.width=12, fig.height=8}
library(glmnet)

rm_vars <- c("pi", "pi_dayfromadmit", "pressure_injury_poa")
x_training = trainingData[, - which(names(trainingData) %in% rm_vars)]
x_test = testData[, - which(names(testData) %in% rm_vars)]
```



```{r fig.width=12, fig.height=8}
cvlog = cv.glmnet(data.matrix(x_training), trainingData$pi, family = "binomial", type.measure = "class")

plot(cvlog)
```

```{r}
library(caret)
y_test_pred <- predict(cvlog, newx = data.matrix(x_test), s = "lambda.min", type = "class")
xtab <- table(as.factor(testData$pi), as.factor(y_test_pred))
confusionMatrix(xtab, positive ="1")
```
### Compare the above result with a dummy prediction

```{r}
y_dummy <- factor(rep("0",nrow(testData)), levels = c("0","1"))
table(y_dummy)
dummytab <- table(as.factor(testData$pi), y_dummy)
dummytab
confusionMatrix(dummytab, positive ="1")
```

```{r}
age <- (static_combined$ageinyears-mean(static_combined$ageinyears))/(max(static_combined$ageinyears) -min(static_combined$ageinyears))
par(mfrow=c(1,3))
plot(density(static_scaled$ageinyears))
plot(density(static_combined$ageinyears))
plot(density(age))
```




